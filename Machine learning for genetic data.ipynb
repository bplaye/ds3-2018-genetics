{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Machine learning for genetic data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Introduction\n",
    "\n",
    "The goal of this practical session is to manipulate high-dimensional, low sample-size data that is typical of many genetic applications.\n",
    "\n",
    "Here we will work with GWAS data from _Arabidopsis thaliana_, which is a plant model organism. The genotypes are hence described by **Single Nucleotide Polymorphisms, or SNPs**. Our goal will be to use this data to identify regions of the genome that can be linked with various growth and flowering traits (**phenotypes**)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data description\n",
    "\n",
    "* `data/athaliana_small.X.txt` is the design matrix. As many rows as samples, as many columns as SNPs\n",
    "* the SNPs are given (in order) in `data/athaliana_small.snps.txt`. \n",
    "* the samples are given (in order) in `data/athaliana.samples.txt`.\n",
    "\n",
    "* the phenotypes are given in `data/phenotypes.pheno`. The first two columns give the sample's ID, and all following columns give a phenotype. The header gives the list of all phenotypes. In this session we will use \"2W\" and \"4W\", which give the number of days by which the plant grows to be 5 centimeters tall, after either two weeks (\"2W\") or four weeks (\"4W\") of vernalization (i.e. the seeds are kept at cold temperatures, similar to winter). Not all phenotypes are available for all samples.\n",
    "\n",
    "* `data/athaliana.snps_by_gene.txt` contains, for each _A. thaliana_ SNP, the list of genes it is in or near to. (This can be several genes, as it is customary to use a rather large window to compute this, so as to capture potential cis-regulatory effects.)\n",
    "\n",
    "* the feature network is in `data/athaliana_small.W.txt`. It has been saved as 3 arrays, corresponding to the row, col, and data attributes of a [scipy.sparse coo_matrix](https://docs.scipy.org/doc/scipy/reference/generated/scipy.sparse.coo_matrix.html)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Loading the data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%pylab inline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Read the list of SNP names"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('data/athaliana_small.snps.txt') as f:\n",
    "    snp_names = f.readline().split()\n",
    "    f.close()\n",
    "print len(snp_names), snp_names[:10]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Read the list of sample names"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "samples = list(np.loadtxt('data/athaliana.samples.txt', # file names\n",
    "                         dtype=int)) # values are integers\n",
    "print len(samples), samples[:10]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Load the design matrix (n samples x p SNPs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = np.loadtxt('data/athaliana_small.X.txt',  # file names\n",
    "               dtype='int') # values are integers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "n, p = X.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Load the 2W phenotype data\n",
    "\n",
    "The first phenotype we will work with is called \"2W\". It describes the number of days required for the bolt height to reach 5 cm, at a temperature of 23°C under 16 hours of daylight per 24 hours, for seeds that have been vernalized for 2 weeks at 5°C (with 8 hours of daylight per 24 hours)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO\n",
    "# read phenotypes from phenotypes.pheno\n",
    "# only keep samples that have a 2W phenotype. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Restrict X to the samples with a 2W phenotype, in correct order\n",
    "# X_2W[i] = X[samples.index(samples_with_phenotype[i])]\n",
    "X_2W = X[np.array([samples.index(sample_id) \\\n",
    "                   for sample_id in samples_with_phenotype]), :]\n",
    "n, p = X_2W.shape\n",
    "print n, p"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# You can delete X now if you want, to free space\n",
    "del X"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Split the data in a train and test set\n",
    "\n",
    "We will set aside a test set, containing 20% of our samples, on which to evaluate the quality of our predictive models."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn import model_selection"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_2W_tr, X_2W_te, y_2W_tr, y_2W_te = \\\n",
    "    model_selection.train_test_split(X_2W, y_2W, test_size=0.2, \n",
    "                                     random_state=17)\n",
    "print X_2W_tr.shape, X_2W_te.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Visualize the phenotype"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "h = plt.hist(y_2W_tr, bins=30)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## T-test\n",
    "\n",
    "Let us start by running a statistical test for association of each SNP feature with the phenotype."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import statsmodels.api as sm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "p = X_2W_tr.shape[1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pvalues = []\n",
    "for snp_idx in range(p):\n",
    "    # only look a the column corresponding at that SNP\n",
    "    X_snp = X_2W_tr[:, snp_idx]\n",
    "    # run a linear regression (with bias) between the phenotype and \n",
    "    # this SNP\n",
    "    X_snp = sm.add_constant(X_snp)\n",
    "    est = sm.regression.linear_model.OLS(y_2W_tr, X_snp)\n",
    "    est2 = est.fit()\n",
    "    # get the p-value from the model \n",
    "    pvalues.append(est2.pvalues[1])\n",
    "pvalues = np.array(pvalues)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Manhattan plot\n",
    "\n",
    "The common way to visualize such results is by using a Manhattan plot: we will plot all SNPs on the x-axis, and on the y-axis we'll have the opposite of the log base 10 of the p-value. The lower the p-value, the higher the corresponding marker. \n",
    "\n",
    "We will also add a horizontal line that corresponds to the _threshold for significance_. Because we are testing multiple hypotheses, we need to lower our threshold accordingly. We will use __Bonferroni correction__ and divide the significance threshold (say, alpha=0.05) by the number of tests, that is, the number of SNPs p."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.scatter(range(p), # x = SNP position\n",
    "            -np.log10(pvalues)) # y = -log10 p-value \n",
    "\n",
    "# significance threshold according to Bonferroni correction\n",
    "t = -np.log10(0.05/p)\n",
    "plt.plot([0, p], [t, t])\n",
    "\n",
    "# plot labels\n",
    "plt.xlabel(\"feature\")\n",
    "plt.ylabel(\"-log10 p-value\")\n",
    "plt.xlim([0, p])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "__What do you observe? Are any SNPs significantly associated with the phenotype? What genes are they in/near?__"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Linear regression "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn import linear_model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_lr = linear_model.LinearRegression(fit_intercept=True)\n",
    "model_lr.fit(X_2W_tr, y_2W_tr)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(6, 4))\n",
    "plt.scatter(range(p), # x = SNP position\n",
    "            model_lr.coef_)  # y = regression weights\n",
    "\n",
    "plt.xlabel(\"SNP\")\n",
    "plt.ylabel(\"regression weight\")\n",
    "plt.xlim([0, p])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "__What do you observe? How can you interpret these results? Do any of the SNPs strike you as having a strong influence on the phenotype?__"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Model predictive power"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn import metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_2W_lr_pred = model_lr.predict(X_2W_te)\n",
    "\n",
    "print \"Percentage of variance explained (using all SNPs): %.2f\" % \\\n",
    "    metrics.explained_variance_score(y_2W_te, y_2W_lr_pred)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(6, 6))\n",
    "plt.scatter(y_2W_te, y_2W_lr_pred)\n",
    "\n",
    "plt.xlabel(\"true phenotype\")\n",
    "plt.ylabel(\"prediction\")\n",
    "plt.xlim([np.min(y_2W_te)-5, np.max(y_2W_te)+5])\n",
    "plt.ylim([np.min(y_2W_te)-5, np.max(y_2W_te)+5])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Lasso"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "alphas = np.logspace(-4., 1., num=20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lasso = linear_model.Lasso(fit_intercept=True)\n",
    "model_l1 = model_selection.GridSearchCV(lasso, \n",
    "                                        param_grid={'alpha': alphas}, \n",
    "                                        scoring='explained_variance')\n",
    "model_l1.fit(X_2W_tr, y_2W_tr)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(6, 4))\n",
    "plt.scatter(range(p), # x = SNP position\n",
    "            model_l1.best_estimator_.coef_)  # y = regression weights\n",
    "\n",
    "plt.xlabel(\"SNP\")\n",
    "plt.ylabel(\"lasso regression weight\")\n",
    "plt.xlim([0, p])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "__How can you interpret these results? How many SNPs contribute to explaining the phenotype?__"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print \"%d SNPs selected.\" % \\\n",
    "    np.nonzero(model_l1.best_estimator_.coef_)[0].shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Predictive power "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_2W_l1_pred = model_l1.best_estimator_.predict(X_2W_te)\n",
    "\n",
    "print \"Percentage of variance explained (using %d SNPs): %.2f\" % \\\n",
    "     (np.nonzero(model_l1.best_estimator_.coef_)[0].shape[0], \n",
    "      metrics.explained_variance_score(y_2W_te, y_2W_l1_pred))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(6, 6))\n",
    "plt.scatter(y_2W_te, y_2W_l1_pred)\n",
    "\n",
    "plt.xlabel(\"true phenotype\")\n",
    "plt.ylabel(\"prediction\")\n",
    "plt.xlim([np.min(y_2W_te)-0.05, np.max(y_2W_te)+0.05])\n",
    "plt.ylim([np.min(y_2W_te)-0.05, np.max(y_2W_te)+0.05])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Stability\n",
    "\n",
    "__How stable is the set of selected SNPs, between the different rounds of cross-validation with optimal parameters?__\n",
    "\n",
    "You can use [sklearn.metrics.jaccard_similarity_score](http://scikit-learn.org/stable/modules/generated/sklearn.metrics.jaccard_similarity_score.html), or implement Kuncheva's consistency index."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "__Note:__ One could also contemplate using the Jaccard similarity (or another measure of consistency/stability/robustness) as a criterion to select the best hyperparameters. Pay attention, however, to the fact that hyperparameters selecting no features at all or all the features will have very good consistency."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Elastic net\n",
    "\n",
    "One solution to make the lasso more stable is to use a combination of the l1 and l2 regularizations.\n",
    "\n",
    "We are now minimizing the loss + a linear combination of an l1-norm and an l2-norm over the regression weights. This imposes sparsity, but encourages correlated features to be selected together, where the lasso would tend to pick only one (at random) of a group of correlated features.\n",
    "\n",
    "The elastic net is implemented in scikit-learn's [linear_model.ElasticNet](http://scikit-learn.org/stable/modules/generated/sklearn.linear_model.ElasticNet.html#sklearn.linear_model.ElasticNet)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Parameters grid\n",
    "alphas = np.logspace(-4., 1., num=15)\n",
    "ratios = np.linspace(0.5, 1., num=3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "enet = linear_model.ElasticNet(fit_intercept=True)\n",
    "model_l1l2 = model_selection.GridSearchCV(enet, \n",
    "                                        param_grid={'alpha': alphas, \n",
    "                                                    'l1_ratio': ratios}, \n",
    "                                        scoring='explained_variance')\n",
    "model_l1l2.fit(X_2W_tr, y_2W_tr)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(6, 4))\n",
    "plt.scatter(range(p), # x = SNP position\n",
    "            model_l1l2.best_estimator_.coef_)  # y = regression weights\n",
    "\n",
    "plt.xlabel(\"SNP\")\n",
    "plt.ylabel(\"elastic net regression weight\")\n",
    "plt.xlim([0, p])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "__How can you interpret these results? How many SNPs contribute to explaining the phenotype?__"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print \"%d SNPs selected.\" % \\\n",
    "    np.nonzero(model_l1l2.best_estimator_.coef_)[0].shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Predictive power "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_2W_l1l2_pred = model_l1.best_estimator_.predict(X_2W_te)\n",
    "\n",
    "print \"Percentage of variance explained (using %d SNPs): %.2f\" % \\\n",
    "     (np.nonzero(model_l1l2.best_estimator_.coef_)[0].shape[0], \n",
    "      metrics.explained_variance_score(y_2W_te, y_2W_l1_pred))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(6, 6))\n",
    "plt.scatter(y_2W_te, y_2W_l1l2_pred)\n",
    "\n",
    "plt.xlabel(\"true phenotype\")\n",
    "plt.ylabel(\"prediction\")\n",
    "plt.xlim([np.min(y_2W_te)-0.05, np.max(y_2W_te)+0.05])\n",
    "plt.ylim([np.min(y_2W_te)-0.05, np.max(y_2W_te)+0.05])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Stability\n",
    "\n",
    "__How stable is the set of selected SNPs, between the different rounds of cross-validation with optimal parameters?__"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Stability selection with the Lasso\n",
    "\n",
    "__Use a randomized procedure to stabilize the lasso__\n",
    "\n",
    "[sklearn.linear_model.RandomizedLasso.html](http://scikit-learn.org/0.18/modules/generated/sklearn.linear_model.RandomizedLasso.html#sklearn.linear_model.RandomizedLasso) + [User Guide](http://scikit-learn.org/0.18/auto_examples/linear_model/plot_sparse_recovery.html)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Network-constrained lasso\n",
    "\n",
    "This is not implemented in scikit-learn, so we'll need to create our own estimator.\n",
    "\n",
    "It turns out that it is possible to transform the network-constrained Lasso problem into a Lasso problem: follow [the original paper](https://academic.oup.com/bioinformatics/article/24/9/1175/206444) (pdf also available [here](http://www.stat.purdue.edu/~doerge/BIOINFORM.D/FALL10/Li_and_Li_2008_Bioinformatics.pdf) and the note in section C of [the supplementary material of Sugiyama et al. (2014)](http://cazencott.info/dotclear/public/publications/sugiyama2014_supp.pdf) to replace the eigen-decomposition of the graph Laplacian with the graph incidence matrix.\n",
    "\n",
    "Follow the [documentation](http://scikit-learn.org/stable/developers/contributing.html#rolling-your-own-estimator) or this [blog post](http://danielhnyk.cz/creating-your-own-estimator-scikit-learn/) to create a scikit-learn estimator.\n",
    "\n",
    "Be careful: the computations might require a lot of RAM."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Load the network"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from scipy import sparse"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "w_saved = np.loadtxt('data/athaliana_small.W.txt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "W = sparse.coo_matrix((w_saved[2, :], (np.array(w_saved[0, :], dtype=int), \n",
    "                                       np.array(w_saved[1, :], dtype=int))), \n",
    "                      shape=(p, p))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Build the incidence matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compute node degrees \n",
    "degrees = np.zeros((p, ))\n",
    "for vertex in W.row:\n",
    "    degrees[vertex] += 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tim = sparse.lil_matrix((W.row.shape[0], p))\n",
    "for ix, edge in enumerate(W.data):\n",
    "    tim[ix, W.row[ix]] = np.sqrt(edge / degrees[W.row[ix]])\n",
    "    tim[ix, W.col[ix]] = - np.sqrt(edge / degrees[W.col[ix]])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Create the network-constrained Lasso class"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn import base, linear_model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ncLasso(base.BaseEstimator, base.RegressorMixin):\n",
    "    def __init__(self, transposed_incidence=None, lambda1=1.0, lambda2=1.0):\n",
    "        self.transposed_incidence = transposed_incidence # sparse matrix\n",
    "        self.lambda1 = lambda1\n",
    "        self.lambda2 = lambda2\n",
    "        \n",
    "    def fit(self, X, y):       \n",
    "        alpha = self.lambda1/(np.sqrt(self.lambda2+1.))\n",
    "        self.lasso = linear_model.Lasso(fit_intercept=True, alpha=alpha)\n",
    "        \n",
    "        \n",
    "        y_new = np.hstack((y, np.zeros((self.transposed_incidence.shape[0], ))))\n",
    "        print y_new.shape, X.shape\n",
    "        X_new = 1/(np.sqrt(self.lambda2+1)) * sparse.vstack((X, np.sqrt(self.lambda2)*\\\n",
    "                                                    self.transposed_incidence))\n",
    "        \n",
    "        \n",
    "        self.lasso.fit(X_new, y_new)\n",
    "        self.coef_ = self.lasso.coef_[:X.shape[1]]/(np.sqrt(self.lambda2+1))\n",
    "        return self\n",
    "        \n",
    "        \n",
    "    def predict(self, X, y=None):\n",
    "        return self.lasso.predict(X)\n",
    "    \n",
    "    \n",
    "    def score(self, X, y=None):\n",
    "        return self.lasso.score(X, y)                                        "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "__Use the network-constrained Lasso on the data. What do you observe?__"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If you want to use SConES/sfan, see [github/chagaz/sfan](https://github.com/chagaz/sfan). The StructuredSparsity notebook can help."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Multi-task feature selection"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1) Repeat the previous analysis for the 4W phenotype. It is very similar to the 2W phenotype, except that the seeds have been vernelized for 4 weeks. \n",
    "\n",
    "2) It is not unreasonable to expect the genomic regions driving both those phenotypes to be (almost) the same. Use the multi-task version of the Lasso, ENet, or ncLasso algorithms to analyzed both phenotypes simultaneously.\n",
    "\n",
    "Use [sklearn.linear_model.MultiTaskLasso](http://scikit-learn.org/stable/modules/generated/sklearn.linear_model.MultiTaskLasso.html#sklearn.linear_model.MultiTaskLasso) + [User Guide](http://scikit-learn.org/stable/auto_examples/linear_model/plot_multi_task_lasso_support.html)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
